# -*- coding: utf-8 -*-
"""Gabarito - Lista PySpark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15jJ8NNcd4jmHqTso95UQ8uS5f_R0fscs
"""

# Importação da biblioteca pandas
import pandas as pd

# Instalação dos requisitos para o PySpark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

# Configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"
# Torna o pyspark "importável"
import findspark
findspark.init()

from pyspark.sql import SparkSession

# Inicializar a SparkSession com suporte ao Hive
spark = SparkSession.builder \
    .appName("Spark with Hive on Colab") \
    .config("spark.sql.catalogImplementation", "hive") \
    .config("spark.sql.warehouse.dir", "/content/spark-warehouse") \
    .config("hive.metastore.warehouse.dir", "/content/spark-warehouse") \
    .enableHiveSupport() \
    .getOrCreate()

# Criar diretório para o warehouse
!mkdir -p /content/spark-warehouse

# Verifica o SparkContext
print(spark)

# Exibe a Spark version
print(spark.version)

'''
Aplicar agregações ou cálculos: Operações como médias, somas e contagens podem
ser realizadas com funções da biblioteca pyspark.sql.functions
(importadas como F).
'''
from pyspark.sql import functions as F

cliente = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQW3fP3j4qoiMGBXDAGzg_9IW2b3zgjdkVKLsURNVe9QezpHXimWfKle_55CQQtkeWL69OAASBDNdk8/pub?gid=2073489257&single=true&output=csv')
display(cliente)

aluguel = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRncyLGO1iOo2H53JaryzVF1GPjUhWl9DsN7TZROCDxaP85iCwl5aW5ffBEzqtpAMRNYkd7eO5ehmgn/pub?gid=1581881382&single=true&output=csv')
display(aluguel)

carro = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSznlX7UXeH_LeKcNteiDnWvdwZyydAQl0_x8NU9cx6G00Zh7SMrjoUuNpytVq7U-iQVzQNJ7jC7GpY/pub?gid=306989914&single=true&output=csv')
display(carro)

marca = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSI-4_QEFwZ6eDGwUDlip0_PGBn7d_F7j59UwXmRvWbQFyy01ENPatjkbO1E8k5ZW5lqSY9ox112j7X/pub?gid=1160143272&single=true&output=csv')
display(marca)

cliente.to_csv('cliente.csv',index=False)

aluguel.to_csv('aluguel.csv',index=False)

marca.to_csv('marca.csv',index=False)

carro.to_csv('carro.csv',index=False)

# Cria a tabela cliente
spark.sql('''
CREATE TABLE IF NOT EXISTS cliente (
  codcliente INT,
  nome STRING,
  cidade STRING,
  sexo STRING,
  estado STRING,
  estadocivil STRING
)

USING CSV
OPTIONS (path '/content/cliente.csv', header 'true', inferSchema 'true')
''')
cliente = spark.sql('''
SELECT *
FROM cliente
''')

# Cria a tabela aluguel
spark.sql('''
CREATE TABLE IF NOT EXISTS aluguel (
  codaluguel INT,
  codcliente INT,
  codcarro INT,
  data_aluguel DATE
)

USING CSV
OPTIONS (path '/content/aluguel.csv', header 'true', inferSchema 'true')
''')
aluguel = spark.sql('''
SELECT *
FROM aluguel
''')

# Cria a tabela carro
spark.sql('''
CREATE TABLE IF NOT EXISTS carro (
  codcarro INT,
  codmarca INT,
  modelo STRING,
  valor DOUBLE
)

USING CSV
OPTIONS (path '/content/carro.csv', header 'true', inferSchema 'true')
''')
carro = spark.sql('''
SELECT *
FROM carro
''')

# Cria a tabela marca
spark.sql('''
CREATE TABLE IF NOT EXISTS marca (
  codmarca INT,
  marca STRING
)

USING CSV
OPTIONS (path '/content/marca.csv', header 'true', inferSchema 'true')
''')
marca = spark.sql('''
SELECT *
FROM marca
''')

'''
01 - Carregar as quatro tabelas do banco locadora no PySpark como DataFrames
'''
aluguel = spark.read.csv("aluguel.csv", header=True, inferSchema=True)
carro = spark.read.csv("carro.csv", header=True, inferSchema=True)
cliente = spark.read.csv("cliente.csv", header=True, inferSchema=True)
marca = spark.read.csv("marca.csv", header=True, inferSchema=True)

'''
02 - Exibir as cinco primeiras linhas de cada DataFrame
'''
print("Primeiras 5 linhas de 'aluguel':")
aluguel.show(5)
print("Primeiras 5 linhas de 'carro':")
carro.show(5)
print("Primeiras 5 linhas de 'cliente':")
cliente.show(5)
print("Primeiras 5 linhas de 'marca':")
marca.show(5)

'''
03 - Contar o número de linhas e colunas de cada tabela
'''
linhas_aluguel = aluguel.count()
colunas_aluguel = len(aluguel.columns)
print(f"'aluguel' tem {linhas_aluguel} linhas e {colunas_aluguel} colunas.")

linhas_carro = carro.count()
colunas_carro = len(carro.columns)
print(f"'carro' tem {linhas_carro} linhas e {colunas_carro} colunas.")

linhas_carro = carro.count()
colunas_carro = len(carro.columns)
print(f"'carro' tem {linhas_carro} linhas e {colunas_carro} colunas.")

linhas_marca = marca.count()
colunas_marca = len(marca.columns)
print(f"'marca' tem {linhas_marca} linhas e {colunas_marca} colunas.")

'''
04 - Exibir o esquema (schema) de cada DataFrame
'''
print("Schema de 'aluguel':")
aluguel.printSchema()

print("Schema de 'carro':")
carro.printSchema()

print("Schema de 'cliente':")
cliente.printSchema()

print("Schema de 'marca':")
marca.printSchema()

'''
05 - Renomear as colunas dos DataFrames para ter nomes mais amigáveis
'''
aluguel_r = aluguel.withColumnRenamed("codaluguel", "id_aluguel") \
                       .withColumnRenamed("codcliente", "id_cliente") \
                       .withColumnRenamed("codcarro", "id_carro") \
                       .withColumnRenamed("data_aluguel", "data_aluguel")

carro_r = carro.withColumnRenamed("codcarro", "id_carro") \
                   .withColumnRenamed("modelo", "modelo") \
                   .withColumnRenamed("valor", "preco_aluguel") \
                   .withColumnRenamed("codmarca", "id_marca")

cliente_r = cliente.withColumnRenamed("codcliente", "id_cliente") \
                       .withColumnRenamed("nome", "nome_completo") \
                       .withColumnRenamed("sexo", "genero") \
                       .withColumnRenamed("estadocivil", "estado_civil") \
                       .withColumnRenamed("estado", "uf")

marca_r = marca.withColumnRenamed("codmarca", "id_marca") \
                   .withColumnRenamed("marca", "nome_marca")

print("Colunas de 'aluguel':", aluguel_r.columns)
print("Colunas de 'carro':", carro_r.columns)
print("Colunas de 'cliente':", cliente_r.columns)
print("Colunas de 'marca':", marca_r.columns)

'''
06 - Selecionar apenas os aluguéis realizados após uma data específica
'''
data_especifica = "2023-04-13"

aluguel.where(F.col("data_aluguel") > data_especifica).show()

'''
07 - Encontrar clientes que residem no estado de "RJ"
'''
cliente.where(F.col("estado") == "RJ").show()

'''
08 - Filtrar carros com valor de aluguel maior que 150
'''
carro.where(F.col("valor") > 150).show()

'''
09 - Selecionar aluguéis onde o cliente é do sexo feminino
'''
aluguel.join(cliente, aluguel.codcliente == cliente.codcliente)\
.where(F.col('sexo') == 'F').select('codaluguel','nome','sexo').show()

'''
10 - Identificar clientes solteiros
'''
cliente.where(F.col("estadocivil") == "S").show()

'''
11 - Realizar um join entre "Aluguel" e "Cliente" para adicionar informações do
cliente ao DataFrame de aluguéis
'''
aluguel.join(cliente, aluguel.codcliente == cliente.codcliente, "inner").show()

'''
12 - Juntar "Carro" e "Marca" para incluir o nome da marca no DataFrame de
carros
'''
carro.join(marca, carro.codmarca == marca.codmarca, "inner").show()

'''
13 - Criar um DataFrame combinando "Aluguel", "Carro" e "Cliente"
'''
aluguel.join(cliente, aluguel.codcliente == cliente.codcliente, "inner")\
.join(carro, aluguel.codcarro == carro.codcarro, "inner").show()

'''
14 - Realizar um join entre "Cliente" e "Carro" com uma condição específica
'''
cliente.join(aluguel, cliente.codcliente == aluguel.codcliente, "inner")\
.join(carro, aluguel.codcarro == carro.codcarro, "inner")\
.select('nome','cidade','sexo','estadocivil','modelo','valor')\
.where(F.col('valor') > 100).show()

'''
15 - Encontrar o valor médio dos carros alugados
'''
aluguel.join(carro, aluguel.codcarro == carro.codcarro)\
.agg(F.round(F.avg('valor'),2).alias('valor_media')).show()

'''
15a - Encontrar o valor médio dos carros para aluguel
'''
carro.agg(F.avg("valor").alias("valor_medio_aluguel")).show()

'''
16 - Calcular o número total de clientes por estado
'''
cliente.groupBy("estado").count().show()

'''
16 - Calcular o número total de clientes por estado
'''
cliente.groupBy('estado','nome').agg(F.count('codcliente')).show()

'''
17 - Identificar a marca mais popular com base nos aluguéis
'''
aluguel.join(carro, aluguel.codcarro == carro.codcarro)\
.join(marca, marca.codmarca == carro.codmarca)\
.groupBy('marca').agg(F.count('codaluguel').alias('qtde_alugueis'))\
.orderBy(F.desc('qtde_alugueis')).show(1)

'''
18 - Determinar o maior e menor valor de aluguel entre os carros
'''
carro.agg(
    F.max("valor").alias("maior_valor"),
    F.min("valor").alias("menor_valor")).show()

'''
19 - Classificar os carros pelo valor do aluguel em ordem decrescente
'''
carro.orderBy('valor', ascending=False).show()

'''
20 - Calcular a diferença em dias entre o aluguel mais recente e o mais antigo
'''
# from pyspark.sql.functions import max, min, datediff
datas_extremas = aluguel.agg(
    F.max("data_aluguel").alias("data_recente"),
    F.min("data_aluguel").alias("data_antiga")
)

# Calcular a diferença em dias
diferenca_dias = datas_extremas.select(
    F.datediff("data_recente", "data_antiga").alias("diferenca_dias"))\
    .collect()[0]['diferenca_dias']

# Exibir o resultado
print(f"Diferença em dias entre o aluguel mais recente e o mais antigo: {diferenca_dias} dias")

'''
21 - Criar uma coluna no DataFrame "Carro" para categorizar os valores de
aluguel
'''
carro.withColumn(
    "categoria_valor",
    F.when(carro.valor > 150, "Alto")
    .when(carro.valor > 100, "Médio")
    .otherwise("Baixo")
).show()

'''
22 - Criar uma coluna no DataFrame "Cliente" para indicar se a cidade é a
capital do estado
'''
# Lista de capitais para simplificação
capitais = ["São Paulo", "Rio de Janeiro"]

# Criar a nova coluna que indica se a cidade é uma capital
cliente_com_capital = cliente.withColumn(
    "cidade_capital",
    F.when(cliente.cidade.isin(capitais), 'Sim').otherwise('Interior')
)

# Exibir o DataFrame com a nova coluna
print("DataFrame de clientes com a nova coluna 'cidade_capital':")
cliente_com_capital.show()

'''
23 - Adicionar uma coluna em "Aluguel" com o valor total do aluguel,
considerando uma taxa fixa de 10%
'''
# Adicionar a coluna 'valor_total' considerando uma taxa fixa de 10%
aluguel_com_valor_total = aluguel.join(carro, aluguel.codcarro == carro.codcarro).withColumn(
    "valor_total",
    F.round(F.col("valor") * 1.10,1)  # Multiplica o valor original pela taxa de 10%
)
aluguel_com_valor_total.select('modelo', 'valor', 'valor_total').show()

'''
24 - Agrupar os aluguéis por cliente e contar o número de carros alugados
'''
aluguel.join(cliente, aluguel.codcliente == cliente.codcliente)\
.groupBy('nome')\
.agg(F.count('codaluguel').alias('qtde_alugueis'))\
.orderBy(F.desc('qtde_alugueis')).show()

# Exemplo básico de script PySpark agendado:
# Carregar os dados
carro = spark.read.csv("carro.csv", header=True, inferSchema=True)

# Realizar transformação simples
carro_transformado = carro.withColumn("valor_corrigido", carro["valor"] * 1.10)

# Salvar resultado
carro_transformado.write.csv("aluguel_transformado.csv", header=True)